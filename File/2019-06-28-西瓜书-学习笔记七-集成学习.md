---
title: '[西瓜书]学习笔记七:集成学习'
date: 2019-06-28 09:26:28
tags:
- 机器学习
- 西瓜书
- 集成学习
mathjax: true
---

<center>这是数据竞赛中最常用的方法</center>
<!-- more -->

集成学习通过构建并结合多个学习器来完成学习任务，有时也被称为多分类器系统，基于委员会的学习等。

下图是集成学习的一般结构：先产生一组“个体学习器”，在使用某种策略将他们结合起来。个体学习器通常由一个现有的学习算法从训练数据当中产生。个体学习器可以由相同或不同的的基学习算法构成。

{% asset_img 1.png %}

# 学习器

将多个学习器进行学习，常可获得比单一学习器显著优越的泛化性能。这对“弱学习器”尤为明显，因此集成学习都是针对弱学习器进行的。

要让集成起来的泛化性能比单个学习器都要好，虽说团结力量大但也有木桶短板理论调皮捣蛋，那如何做到呢？这就引出了集成学习的两个重要概念：准确性和多样性（diversity）。准确性指的是个体学习器不能太差，要有一定的准确度；多样性则是个体学习器之间的输出要具有差异性。通过下面的这三个例子可以很容易看出这一点，准确度较高，差异度也较高，可以较好地提升集成性能。

{% asset_img 2.png %}

考虑二分类问题，假设真实函数f，基本分类器的错误率为$\epsilon$，对每个基分类器有
$$
P(h_{i}(x)\neq f(x))=\epsilon
$$

假设集成通过简单投票结合T个基分类器，若超过半数的基分类器正确，则集成分类就正确：
$$
H(x)=sign(\sum_{i=1}^{T}h_{i}(x))
$$

上面的分析有一个关键假设：基学习器的误差相互独立。可现实任务中，个体学习器是解决同一个问题训练出来的，显然不可能相互独立。个体学习器的**准确性**和**多样性**本身就存在冲突，如何产生并结合“好而不同”的个体学习器，是集成学习研究的核心。

现阶段有三种主流的集成学习方法：Boosting、Bagging以及随机森林（Random Forest），接下来将进行逐一介绍。

# Boosting

关于boosting的章节，在我之前的博客中有介绍。{% post_link AdaBoost算法原理 AdaBoost算法原理 %}。所以此处就不再赘述相关问题。

# Bagging与随机森林



# 结合策略

# 多样性问题